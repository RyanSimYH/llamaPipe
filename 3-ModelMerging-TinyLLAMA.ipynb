{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b728f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "testingMode = False\n",
    "\n",
    "endpoint = \"XXX\" if testingMode else os.environ.get('AWS_S3_ENDPOINT')\n",
    "bucket_name = \"XXX\" if testingMode else os.environ.get('AWS_S3_BUCKET')\n",
    "key_id = \"XXX\" if testingMode else os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = \"XXX\" if testingMode else os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3',endpoint_url=endpoint,aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3',endpoint_url=endpoint,aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "\n",
    "def download_s3_folder(s3_folder, local_dir):\n",
    "    s3 =  boto3.client('s3',endpoint_url=endpoint,aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "    # List all objects within the specified S3 folder\n",
    "    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_folder)['Contents']\n",
    "    \n",
    "    for obj in objects:\n",
    "        # Extract the file path from the object key\n",
    "        s3_key = obj['Key']\n",
    "        \n",
    "        # Create the local file path\n",
    "        relative_path = os.path.relpath(s3_key, s3_folder)\n",
    "        local_file_path = os.path.join(local_dir, relative_path)\n",
    "        \n",
    "        # Ensure local directory exists\n",
    "        local_dir_path = os.path.dirname(local_file_path)\n",
    "        if not os.path.exists(local_dir_path):\n",
    "            os.makedirs(local_dir_path)\n",
    "        \n",
    "        # Download the file from S3\n",
    "        s3.download_file(bucket_name, s3_key, local_file_path)\n",
    "        print(f\"Downloaded {s3_key} to {local_file_path}\")\n",
    "    \n",
    "def upload_s3_folder(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                s3_key = os.path.relpath(file_path, folder_path)\n",
    "                try:\n",
    "                    s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "                    print(f'Successfully uploaded {file_path} to s3://{bucket_name}/{s3_key}')\n",
    "                except FileNotFoundError:\n",
    "                    print(f'The file {file_path} was not found')\n",
    "                except NoCredentialsError:\n",
    "                    print('Credentials not available')\n",
    "                except PartialCredentialsError:\n",
    "                    print('Incomplete credentials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Pipeline Training Args\n",
    "import json\n",
    "\n",
    "download_s3_folder(\"PipelineArgs\",\"PipelineArgs\")\n",
    "\n",
    "with open(\"PipelineArgs/PipelineArgs.json\", \"r\") as file:\n",
    "    pipelineArgs = json.load(file)\n",
    "\n",
    "required_keys = [\"MODEL_NAME\", \"HF_MODEL_REPO_ID\", \"HF_DATASET_ID\",\"SYSTEM_INST\"]\n",
    "\n",
    "# Check if the required keys are present in the JSON data\n",
    "if all(key in pipelineArgs for key in required_keys):\n",
    "    print(\"The JSON file contains all the required keys.\")\n",
    "else:\n",
    "    missing_keys = [key for key in required_keys if key not in pipelineArgs]\n",
    "    print(f\"The JSON file is missing the following keys: {', '.join(missing_keys)}\")\n",
    "\n",
    "print(\"Pipeline Arguments downloaded and verified\")\n",
    "print(pipelineArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git llamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft torch==2.3.1 transformers\n",
    "!pip install -r llamacpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_s3_folder(f\"\"\"{pipelineArgs[\"MODEL_NAME\"]}/{pipelineArgs[\"HF_MODEL_REPO_ID\"].split('/')[1]}\"\"\",\"Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer)\n",
    "\n",
    "modelName = os.path.abspath(\"Models/BaseModel\")\n",
    "finetuned_model_id = os.path.abspath(\"Models/TrainedLoRA\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelName,\n",
    "    device_map = 'auto',\n",
    "    token = False,\n",
    ")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    finetuned_model_id,\n",
    "    from_transformers=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained( modelName )\n",
    "model = peft_model.merge_and_unload()\n",
    "model.save_pretrained( \"merged\" )\n",
    "tokenizer.save_pretrained( \"merged\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c931b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llamacpp/convert_hf_to_gguf.py merged --outfile trainedModel.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = f\"\"\"Upload/{pipelineArgs[\"MODEL_NAME\"]}/{pipelineArgs[\"HF_MODEL_REPO_ID\"].split('/')[1]}/ollama\"\"\"\n",
    "os.makedirs(dirpath,exist_ok=True)\n",
    "os.rename(\"trainedModel.gguf\", f\"\"\"{dirpath}/{pipelineArgs[\"MODEL_NAME\"]}.gguf\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"FROM /mnt/models/{pipelineArgs[\"MODEL_NAME\"]}.gguf\"\"\"+\"\"\"\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER stop \"<|im_start|>\"\n",
    "PARAMETER stop \"<|im_end|>\"\n",
    "TEMPLATE \\\"\"\"\n",
    "<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\\\"\"\"\n",
    "\"\"\" + f\"\"\"SYSTEM \\\"\"\"{pipelineArgs[\"SYSTEM_INST\"]}\"\"\\\"\"\"\"\n",
    "\n",
    "with open(f\"{dirPath}/Modelfile\", \"w\") as mf:\n",
    "    mf.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_s3_folder(\"Upload\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
