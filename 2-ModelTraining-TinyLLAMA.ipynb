{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12307bc9-87e6-4653-8c63-00fb17da91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "testingMode = False\n",
    "\n",
    "endpoint = \"XXX\" if testingMode else os.environ.get('AWS_S3_ENDPOINT')\n",
    "bucket_name = \"XXX\" if testingMode else os.environ.get('AWS_S3_BUCKET')\n",
    "key_id = \"XXX\" if testingMode else os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = \"XXX\" if testingMode else os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3',endpoint_url=endpoint,aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3',endpoint_url=endpoint,aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "\n",
    "def download_s3_folder(s3_folder, local_dir):\n",
    "    s3 =  boto3.client('s3',endpoint_url=endpoint,aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "    # List all objects within the specified S3 folder\n",
    "    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_folder)['Contents']\n",
    "    \n",
    "    for obj in objects:\n",
    "        # Extract the file path from the object key\n",
    "        s3_key = obj['Key']\n",
    "        \n",
    "        # Create the local file path\n",
    "        relative_path = os.path.relpath(s3_key, s3_folder)\n",
    "        local_file_path = os.path.join(local_dir, relative_path)\n",
    "        \n",
    "        # Ensure local directory exists\n",
    "        local_dir_path = os.path.dirname(local_file_path)\n",
    "        if not os.path.exists(local_dir_path):\n",
    "            os.makedirs(local_dir_path)\n",
    "        \n",
    "        # Download the file from S3\n",
    "        s3.download_file(bucket_name, s3_key, local_file_path)\n",
    "        print(f\"Downloaded {s3_key} to {local_file_path}\")\n",
    "    \n",
    "def upload_s3_folder(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                s3_key = os.path.relpath(file_path, folder_path)\n",
    "                try:\n",
    "                    s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "                    print(f'Successfully uploaded {file_path} to s3://{bucket_name}/{s3_key}')\n",
    "                except FileNotFoundError:\n",
    "                    print(f'The file {file_path} was not found')\n",
    "                except NoCredentialsError:\n",
    "                    print('Credentials not available')\n",
    "                except PartialCredentialsError:\n",
    "                    print('Incomplete credentials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Pipeline Training Args\n",
    "import json\n",
    "\n",
    "download_s3_folder(\"PipelineArgs\",\"PipelineArgs\")\n",
    "\n",
    "with open(\"PipelineArgs/PipelineArgs.json\", \"r\") as file:\n",
    "    pipelineArgs = json.load(file)\n",
    "\n",
    "required_keys = [\"MODEL_NAME\", \"HF_MODEL_REPO_ID\", \"HF_DATASET_ID\",\"SYSTEM_INST\"]\n",
    "\n",
    "# Check if the required keys are present in the JSON data\n",
    "if all(key in pipelineArgs for key in required_keys):\n",
    "    print(\"The JSON file contains all the required keys.\")\n",
    "else:\n",
    "    missing_keys = [key for key in required_keys if key not in pipelineArgs]\n",
    "    print(f\"The JSON file is missing the following keys: {', '.join(missing_keys)}\")\n",
    "\n",
    "print(\"Pipeline Arguments downloaded and verified\")\n",
    "print(pipelineArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Model from S3\n",
    "download_s3_folder(f\"\"\"{pipelineArgs[\"MODEL_NAME\"]}/{pipelineArgs[\"HF_MODEL_REPO_ID\"].split('/')[1]}/BaseModel\"\"\",\"BaseModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca3bbb-643c-4fd8-9b9d-d45bf1d4aa39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install peft accelerate torch==2.3.1 transformers datasets trl sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a43c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from peft.utils.other import prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "modelName = os.path.abspath(\"BaseModel\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelName,\n",
    "    device_map = 'auto',\n",
    "    token = False,\n",
    ")\n",
    "model = prepare_model_for_kbit_training( model )\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 16,\n",
    "    bias = \"none\",\n",
    "    lora_dropout = 0.05, # Conventional\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")\n",
    "model.add_adapter( peft_config )\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained( modelName, use_fast=False )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if os.path.isdir( \"./temp\" ):\n",
    "    shutil.rmtree( \"./temp\" )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"./temp\",\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    save_strategy = \"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate= 2e-4,\n",
    "    fp16= True,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "PROMPT = pipelineArgs[\"SYSTEM_INST\"]\n",
    "\n",
    "def formatting_func( example ):\n",
    "    example['text'] = f\"\"\"<|im_start|>user\\n{PROMPT} {example[pipelineArgs[\"DATASET_PROMPT_COL_NAME\"]]} <|im_end|>\\n<|im_start|>assistant\\n{example[pipelineArgs[\"DATASET_ANS_COL_NAME\"]]}<|im_end|>\"\"\"\n",
    "    return example\n",
    "\n",
    "def generate_and_tokenize_prompt( prompt ):\n",
    "    return tokenizer( formatting_func( prompt ), truncation = True, max_length = 2048 )\n",
    "\n",
    "dataset = load_dataset(pipelineArgs[\"HF_DATASET_ID\"],split=\"train\")\n",
    "dataset = dataset.select(range(200)).shuffle(seed=65).train_test_split(test_size=0.2)\n",
    "train_data = dataset['train'].map(formatting_func)\n",
    "valid_data = dataset['test'].map(formatting_func)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if os.path.isdir( \"./temp\" ):\n",
    "    shutil.rmtree( \"./temp\" )\n",
    "\n",
    "directory = f\"\"\"Upload/{pipelineArgs[\"MODEL_NAME\"]}/{pipelineArgs[\"HF_MODEL_REPO_ID\"].split('/')[1]}/TrainedLoRA\"\"\"\n",
    "\n",
    "if os.path.isdir( directory ):\n",
    "    shutil.rmtree( directory )\n",
    "\n",
    "trainer.model.save_pretrained( directory )\n",
    "print( f\"Model saved '{directory}'.\" )\n",
    "\n",
    "upload_s3_folder(\"Upload\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
